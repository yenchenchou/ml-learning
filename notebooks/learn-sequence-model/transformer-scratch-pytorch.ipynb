{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch with Pytorch\n",
    "\n",
    "This is a Transformer implementation using Pytorch. This is more like a personal understanding than fully optimized version. The example will use `airline-passengers` as reference data to make sure the model is working properly by running both Torch bidrectional with multi-layer LSTM and the hand made version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "os.chdir(\"/Users/yenchenchou/Documents/GitHub/ml-learning\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvInit:\n",
    "    def available_device(self) -> torch.device:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        return device\n",
    "\n",
    "    def fix_seed(self, seed: int) -> int:\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.manual_seed(seed)\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_len, _ = q.size()\n",
    "        query = (\n",
    "            self.wq(q).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        )  # (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
    "        key = (\n",
    "            self.wk(k).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        )  # (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
    "        value = (\n",
    "            self.wv(v).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        )  # (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
    "        attention_score = (\n",
    "            query @ key.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        )  # (batch, n_heads, seq_len, d_k) @ (batch, n_heads, d_k, seq_len) -> (batch, n_heads, seq_len, seq_len)\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attn = self.softmax(attention_score)\n",
    "        # (batch, n_heads, seq_len, seq_len) @ (batch, n_heads, seq_len, d_k) -> (batch, n_heads, seq_len, d_k)\n",
    "        attn_output = attn @ value\n",
    "        # (batch, n_heads, seq_len, seq_len) -> (batch, seq_len, n_heads, seq_len)\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_len, self.d_model)\n",
    "        )\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        attn_output = self.wo(attn_output)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True,  True, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(3, 3), diagonal=1) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 4])\n",
      "tensor([[[[[False, False,  True,  True],\n",
      "           [False, False, False,  True]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[False, False,  True,  True],\n",
      "           [False,  True,  True,  True]]]]])\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq, pad_token=0):\n",
    "    mask = (seq == pad_token).unsqueeze(1).unsqueeze(2)\n",
    "    return mask  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "# Example usage\n",
    "seq = torch.tensor(\n",
    "    [\n",
    "        [[7, 6, 0, 0], [1, 2, 3, 0]],\n",
    "        [[7, 6, 0, 0], [1, 0, 0, 0]]\n",
    "    ]\n",
    ")\n",
    "print(seq.shape)\n",
    "padding_mask = create_padding_mask(seq)\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9854, -0.1618, -0.2974],\n",
      "         [ 1.9854, -0.1618, -0.2974]],\n",
      "\n",
      "        [[ 0.5067,  2.3083, -1.2554],\n",
      "         [ 0.5067,  2.3083, -1.2554]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "tmp = torch.stack([x, x], dim=1)\n",
    "print(tmp)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 7., 0.],\n",
      "        [9., 8., 5.]])\n",
      "tensor([5.3333, 7.3333])\n",
      "tensor([9.0000, 7.5000, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 10, (2, 3)).float()\n",
    "print(x)\n",
    "print(torch.mean(x, dim=1))\n",
    "print(torch.mean(x, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 0.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7003, -1.9755, -0.6597]],\n",
      "\n",
      "        [[-0.7003, -1.9755, -0.6597]]])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "tmp = torch.stack([x, x], dim=0)\n",
    "print(tmp)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def create_sequence_mask(seq):\n",
    "    seq_len = seq.size(1)\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# Example usage\n",
    "seq_len = 4\n",
    "sequence_mask = create_sequence_mask(torch.zeros(seq_len, seq_len))\n",
    "print(sequence_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "# Example usage\n",
    "look_ahead_mask = create_look_ahead_mask(4)\n",
    "print(look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # seq: (batch_size, seq_len)\n",
    "    # Creates a binary mask where padding tokens are 0\n",
    "    return (seq != 0).unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(seq_len):\n",
    "    # Creates a causal mask with an upper triangular matrix of 0s\n",
    "    return torch.tril(torch.ones((seq_len, seq_len))).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Mask:\n",
      " tensor([[[[1, 1, 1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 0]]]], dtype=torch.int32)\n",
      "Padding Mask:\n",
      " tensor([[1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Define sequences with padding\n",
    "input_seq = torch.tensor([\n",
    "    [4, 5, 6, 0, 0],\n",
    "    [3, 8, 9, 2, 0]\n",
    "])\n",
    "\n",
    "# Create padding mask\n",
    "padding_mask = create_padding_mask(input_seq)\n",
    "print(\"Padding Mask:\\n\", padding_mask.int())\n",
    "print(\"Padding Mask:\\n\", padding_mask.squeeze().int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look-Ahead Mask:\n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Define sequence length for look-ahead mask\n",
    "seq_len = 4\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "print(\"Look-Ahead Mask:\\n\", look_ahead_mask.squeeze().int())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

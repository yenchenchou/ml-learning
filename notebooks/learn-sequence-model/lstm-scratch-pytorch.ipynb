{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/yenchenchou/Documents/GitHub/ml-learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvInit:\n",
    "    def available_device(self) -> torch.device:\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        return device\n",
    "\n",
    "    def fix_seed(self, seed: int) -> int:\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.manual_seed(seed)\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MytorchLSTMV1(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom LSTM implementation that follows PyTorch's API.\n",
    "    This LSTM cell includes three main gates: forget, input, and output gates.\n",
    "\n",
    "    (1) Forget gate: Decides what to forget from the cell state.\n",
    "        ft = sigmoid(Ht-1 @ Whf + X @ Wif + bhf) -> prepare what to keep/forget\n",
    "        ft * ct-1 -> actual forgetting\n",
    "\n",
    "    (2) Input gate: Decides what new information to store in the cell state.\n",
    "        it = sigmoid(Ht-1 @ Whi + X @ Wii + bhi) -> prepare what to add\n",
    "        ct_candidate = tanh(Ht-1 @ Whc + X @ Wic + bhc) -> prepare candidate information\n",
    "        it * ct_candidate -> actual adding\n",
    "\n",
    "        ct = ft * ct-1 + it * ct_candidate -> combining forget and input operations to update cell state\n",
    "\n",
    "    (3) Output gate: Decides what information to output based on the new cell state.\n",
    "        ot = sigmoid(Ht-1 @ Who + X @ Wio + bho)\n",
    "        ht = ot * tanh(ct) -> applying non-linearity to the cell state and filtering to produce the new hidden state\n",
    "\n",
    "    The implementation uses the following notation:\n",
    "        - Ht-1: Previous hidden state\n",
    "        - X: Current input\n",
    "        - ct-1: Previous cell state\n",
    "        - Wif, Wii, Wic, Wio: Weight matrices for input to gates\n",
    "        - Whf, Whi, Whc, Who: Weight matrices for hidden state to gates\n",
    "        - bif, bii, bic, bio: Bias vectors for input to gates\n",
    "        - bhf, bhi, bhc, bho: Bias vectors for hidden state to gates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, batch_first: bool = False, bias: bool = True\n",
    "    ):\n",
    "        super(MytorchLSTMV1, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Forget gate\n",
    "        self.wif = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.whf = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # Input gate\n",
    "        self.wii = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.whi = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # Candidate gate\n",
    "        self.wic = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.whc = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        # Output gate\n",
    "        self.wio = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.who = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, hidden_tuple: tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n",
    "        if self.batch_first:\n",
    "            X = X.transpose(0, 1)\n",
    "\n",
    "        h_prev, c_prev = hidden_tuple\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(X.size(0)):\n",
    "            x_t = X[t]\n",
    "            ft = torch.sigmoid(self.wif(x_t) + self.whf(h_prev))\n",
    "            it = torch.sigmoid(self.wii(x_t) + self.whi(h_prev))\n",
    "            candidate = torch.tanh(self.wic(x_t) + self.whc(h_prev))\n",
    "            c_prev = ft * c_prev + it * candidate\n",
    "            ot = torch.sigmoid(self.wio(x_t) + self.who(h_prev))\n",
    "            h_prev = ot * torch.tanh(c_prev)\n",
    "            outputs.append(h_prev)\n",
    "\n",
    "        outputs = torch.cat(outputs)\n",
    "        if self.batch_first:\n",
    "            outputs = outputs.transpose(0, 1)\n",
    "\n",
    "        return outputs, (h_prev, c_prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_size = 6\n",
    "batch_size = 2\n",
    "seq_length = 8\n",
    "lstm = MytorchLSTMV1(input_size, hidden_size, batch_first=True, bias=True)  # input_size, hidden_size\n",
    "x = torch.randn(batch_size, seq_length, input_size)  # batch_size, seq_length, input_size\n",
    "h0 = torch.randn(batch_size, hidden_size).unsqueeze(0)  # batch_size, hidden_size -> 1(one_directional), batch_size, hidden_size\n",
    "c0 = torch.randn(batch_size, hidden_size).unsqueeze(0)  # batch_size, hidden_size -> 1(one_directional), batch_size, hidden_size\n",
    "output, (ht, ct) = lstm(x, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0486, -0.4845, -0.1612,  0.0953, -0.4194, -0.4590],\n",
       "          [ 0.0302, -0.2870, -0.0870,  0.0015, -0.4005, -0.0403],\n",
       "          [ 0.1063, -0.1486, -0.0224, -0.0485, -0.2485,  0.0127],\n",
       "          [ 0.1612, -0.1206, -0.2458, -0.0148, -0.2493,  0.1344],\n",
       "          [ 0.1405, -0.2365, -0.1662, -0.1215,  0.0457,  0.0251],\n",
       "          [ 0.2077, -0.1192, -0.1891, -0.0703, -0.0832,  0.0990],\n",
       "          [ 0.2366, -0.0976,  0.0934, -0.2086,  0.0132,  0.0281],\n",
       "          [ 0.1891, -0.1331,  0.1026, -0.1295,  0.0250,  0.0550]],\n",
       " \n",
       "         [[-0.3546, -0.4126, -0.2590, -0.2608,  0.0372, -0.3793],\n",
       "          [-0.0580, -0.0429, -0.3364, -0.0255,  0.0847,  0.0588],\n",
       "          [ 0.0006, -0.0506, -0.2623, -0.0594, -0.0038,  0.1337],\n",
       "          [ 0.0991, -0.1199, -0.0696, -0.1217,  0.0202,  0.1158],\n",
       "          [ 0.1514, -0.0383, -0.1470, -0.0240, -0.2287,  0.1310],\n",
       "          [ 0.2270, -0.1361, -0.0942, -0.0718,  0.0182,  0.1390],\n",
       "          [ 0.1693, -0.1397, -0.2141, -0.0412, -0.0319,  0.1919],\n",
       "          [ 0.1453, -0.1554, -0.2053, -0.0744,  0.0076,  0.1989]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " torch.Size([2, 8, 6]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1891, -0.1331,  0.1026, -0.1295,  0.0250,  0.0550],\n",
       "          [ 0.1453, -0.1554, -0.2053, -0.0744,  0.0076,  0.1989]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " torch.Size([1, 2, 6]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht, ht.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6705, -0.2353,  0.1591, -0.2561,  0.0606,  0.1123],\n",
       "          [ 0.5622, -0.2613, -0.3081, -0.1608,  0.0177,  0.3928]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([1, 2, 6]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct, ct.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1, batch_first=True)\n",
    "input = torch.randn(3, 5, 10)\n",
    "h0 = torch.randn(3, 20).unsqueeze(0)\n",
    "c0 = torch.randn(3, 20).unsqueeze(0)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 6])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 20])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 20])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
